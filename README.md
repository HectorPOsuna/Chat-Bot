# ðŸ¤– Chatbot IA con Node.js + Ollama (Dockerizado)

Este proyecto implementa un **chatbot de Inteligencia Artificial** utilizando:

- **Ollama** â†’ para ejecutar modelos LLM localmente (phi3, llama3, mistral, etc.)
- **Node.js + Express** â†’ como API REST del chatbot
- **Docker + Docker Compose** â†’ para que cualquier persona pueda ejecutarlo sin instalar Ollama ni Node.js localmente

El entorno completo corre en contenedores, lo que permite facilidad de uso, portabilidad y despliegue sencillo.

---

# ðŸš€ CaracterÃ­sticas

- Chatbot IA completamente funcional
- Backend en Node.js consumiendo Ollama por medio de HTTP
- Docker Compose para levantar ambos servicios simultÃ¡neamente
- Ollama con modelo `phi3` incluido
- API REST lista para consumir desde Postman, frontend o mÃ³vil
- Compatible con Windows / Linux / Mac (solo usa Docker)
- Totalmente local y gratuito

---

# ðŸ— Arquitectura del Proyecto

chatbot/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â”œâ”€â”€ index.js
â”‚ â”œâ”€â”€ package.json
â”‚ â””â”€â”€ package-lock.json
â”œâ”€â”€ ollama/
â”‚ â””â”€â”€ Dockerfile


### Servicios

| Servicio | Puerto | DescripciÃ³n |
|---------|--------|-------------|
| **Ollama** | 11434 | Ejecuta el modelo LLM dentro de Docker |
| **Backend Node.js** | 3000 | API REST que recibe mensajes y responde con la IA |

---

# ðŸ§© Requisitos previos

Antes de iniciar, necesitas:

- **Docker**: https://www.docker.com/get-started  
- **Docker Compose** (ya viene incluido en Docker Desktop)

No necesitas instalar:

âŒ Node.js  
âŒ Ollama  
âŒ NPM  
âŒ Modelos adicionales  

Todo estÃ¡ dentro de Docker.

---

# âš™ InstalaciÃ³n y uso

ðŸ”§ ðŸŸ¦ PASO 1 â€” Clonar el repositorio
git clone https://github.com/HectorPOsuna/Chat-Bot
cd Chat-Bot

ðŸ³ ðŸŸ¦ PASO 2 â€” Iniciar Ãºnicamente el servicio de Ollama
docker compose up -d ollama

Verificar el estado inicial:
docker logs -f ollama

â³ ðŸŸ¦ PASO 3 â€” Esperar a que Ollama estÃ© listo
until docker exec ollama curl -s http://localhost:11434/api/tags > /dev/null 2>&1; do
  echo "Ollama no responde todavÃ­a... reintentando...";
  sleep 3;
done

ðŸ§  ðŸŸ¦ PASO 4 â€” Precargar el modelo (llama3.2:3b)
docker exec -it ollama ollama pull llama3.2:3b

Verificar que se descargÃ³ correctamente:
docker exec -it ollama ollama list

DeberÃ­a aparecer:
llama3.2:3b

ðŸš€ ðŸŸ¦ PASO 5 â€” Construir e iniciar todos los servicios
docker compose up -d --build

Verificar que ambos contenedores estÃ¡n activos:
docker ps

ðŸ“¡ ðŸŸ¦ PASO 6 â€” Probar la API del chatbot
curl -X POST http://localhost:3000/chat \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hola chatbot, Â¿estÃ¡s funcionando?"}'


