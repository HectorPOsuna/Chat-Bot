# ü§ñ Chatbot IA con Node.js + Ollama (Dockerizado)

Este proyecto implementa un **chatbot de Inteligencia Artificial** completamente funcional con soporte para **contexto conversacional**, utilizando:

- **Ollama** ‚Üí Ejecuta modelos LLM localmente (llama3.1, phi3, mistral, etc.)
- **Node.js + Express** ‚Üí API REST del chatbot con gesti√≥n de contexto
- **Docker + Docker Compose** ‚Üí Despliegue simplificado sin instalaciones locales

El entorno completo corre en contenedores, garantizando portabilidad, facilidad de uso y despliegue sencillo.

---

## üöÄ Caracter√≠sticas

‚úÖ **Chatbot IA completamente funcional** con respuestas en formato Markdown  
‚úÖ **Contexto conversacional** - Mantiene el historial de la conversaci√≥n  
‚úÖ **Respuestas estructuradas** - Formato JSON estandarizado  
‚úÖ **Detecci√≥n autom√°tica de entorno** - Docker o local  
‚úÖ **API REST** lista para consumir desde cualquier cliente  
‚úÖ **Retrocompatible** - Funciona con y sin historial  
‚úÖ **Stateless** - No requiere base de datos ni sesiones  
‚úÖ **Compatible** con Windows / Linux / Mac  
‚úÖ **100% local y gratuito**

---

## üèó Arquitectura del Proyecto

```
chat-bot/
‚îú‚îÄ‚îÄ docker-compose.yml          # Orquestaci√≥n de servicios
‚îú‚îÄ‚îÄ server/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ index.js                # Servidor principal
‚îÇ   ‚îú‚îÄ‚îÄ contextManager.js       # Gesti√≥n de historial
‚îÇ   ‚îú‚îÄ‚îÄ promptBuilder.js        # Construcci√≥n de prompts
‚îÇ   ‚îú‚îÄ‚îÄ responseFormatter.js    # Formato de respuestas
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ package-lock.json
‚îî‚îÄ‚îÄ ollama/
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îî‚îÄ‚îÄ entrypoint.sh
```

### Servicios

| Servicio | Puerto | Descripci√≥n |
|---------|--------|-------------|
| **Ollama** | 11434 | Ejecuta el modelo LLM (llama3.1:latest) |
| **API Server** | 3000 | API REST con soporte de contexto conversacional |

---

## üß© Requisitos Previos

Antes de iniciar, necesitas:

- **Docker**: https://www.docker.com/get-started  
- **Docker Compose** (incluido en Docker Desktop)

**No necesitas instalar:**

‚ùå Node.js  
‚ùå Ollama  
‚ùå NPM  
‚ùå Modelos adicionales  

Todo est√° dentro de Docker.

---

## ‚öôÔ∏è Instalaci√≥n y Uso

### üîß PASO 1 ‚Äî Clonar el repositorio
```bash
git clone https://github.com/HectorPOsuna/Chat-Bot
cd Chat-Bot
```

### üê≥ PASO 2 ‚Äî Iniciar el servicio de Ollama
```bash
docker compose up -d ollama
```

Verificar el estado:
```bash
docker logs -f ollama
```

### ‚è≥ PASO 3 ‚Äî Esperar a que Ollama est√© listo
```bash
until docker exec ollama curl -s http://localhost:11434/api/tags > /dev/null 2>&1; do
  echo "Ollama no responde todav√≠a... reintentando...";
  sleep 3;
done
```

### üß† PASO 4 ‚Äî Precargar el modelo (llama3.1:latest)
```bash
docker exec -it ollama ollama pull llama3.1:latest
```

Verificar que se descarg√≥ correctamente:
```bash
docker exec -it ollama ollama list
```

Deber√≠a aparecer: `llama3.1:latest`

### üöÄ PASO 5 ‚Äî Construir e iniciar todos los servicios
```bash
docker compose up -d --build
```

Verificar que ambos contenedores est√°n activos:
```bash
docker ps
```

---

## üì° Documentaci√≥n de la API

### Endpoint Principal

**URL:** `POST http://localhost:3000/chat`  
**Content-Type:** `application/json`

### Formato de Request

#### Sin Contexto (Mensaje Simple)
```json
{
  "message": "¬øQu√© es Node.js?"
}
```

#### Con Contexto (Conversaci√≥n Continua)
```json
{
  "message": "¬øCu√°l fue el segundo consejo?",
  "history": [
    {
      "role": "user",
      "content": "Dame 3 consejos para programar mejor"
    },
    {
      "role": "assistant",
      "content": "1. **Escribe c√≥digo limpio**\n2. **Usa control de versiones**\n3. **Escribe tests**"
    }
  ]
}
```

### Formato de Response

#### Respuesta Exitosa
```json
{
  "status": "success",
  "data": {
    "reply": "**Node.js** es un entorno de ejecuci√≥n...",
    "model": "llama3.1:latest",
    "timestamp": "2025-11-30T12:00:00.000Z"
  }
}
```

#### Respuesta de Error
```json
{
  "status": "error",
  "error": {
    "message": "Falta 'message' en el body",
    "details": null,
    "timestamp": "2025-11-30T12:00:00.000Z"
  }
}
```

### Par√°metros

| Par√°metro | Tipo | Requerido | Descripci√≥n |
|-----------|------|-----------|-------------|
| `message` | string | ‚úÖ S√≠ | El mensaje del usuario |
| `history` | array | ‚ùå No | Historial de mensajes previos |

### Formato del Historial

El array `history` debe contener objetos con:

- **`role`**: `"user"` o `"assistant"`
- **`content`**: Texto del mensaje

---

## üí° Ejemplos de Uso

### Ejemplo 1: Mensaje Simple (Sin Contexto)

**Request:**
```bash
curl -X POST http://localhost:3000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Expl√≠came qu√© es Docker"}'
```

**Response:**
```json
{
  "status": "success",
  "data": {
    "reply": "**Docker** es una plataforma de contenedores...",
    "model": "llama3.1:latest",
    "timestamp": "2025-11-30T12:00:00.000Z"
  }
}
```

### Ejemplo 2: Conversaci√≥n con Contexto

**Primera petici√≥n:**
```bash
curl -X POST http://localhost:3000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Dame 5 lenguajes de programaci√≥n populares"}'
```

**Segunda petici√≥n (con historial):**
```bash
curl -X POST http://localhost:3000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "¬øCu√°l de esos es mejor para backend?",
    "history": [
      {"role": "user", "content": "Dame 5 lenguajes de programaci√≥n populares"},
      {"role": "assistant", "content": "1. Python\n2. JavaScript\n3. Java\n4. C++\n5. Go"}
    ]
  }'
```

---

## üîß Configuraci√≥n Avanzada

### Variables de Entorno

| Variable | Valor por Defecto | Descripci√≥n |
|----------|-------------------|-------------|
| `OLLAMA_URL` | `http://localhost:11434` | URL del servicio Ollama |

### Cambiar el Modelo

Edita `server/index.js` y cambia:
```javascript
model: "llama3.1:latest"
```

Por el modelo que prefieras (debe estar descargado en Ollama).

---

## üõ† Desarrollo Local (Sin Docker)

Si prefieres ejecutar sin Docker:

1. **Instala Ollama**: https://ollama.ai/
2. **Descarga el modelo**:
   ```bash
   ollama pull llama3.1:latest
   ```
3. **Instala dependencias**:
   ```bash
   cd server
   npm install
   ```
4. **Ejecuta el servidor**:
   ```bash
   node index.js
   ```

El servidor detectar√° autom√°ticamente `http://localhost:11434`.

---

## üìö M√≥dulos del Servidor

### `index.js`
Servidor principal que maneja las peticiones HTTP y coordina los m√≥dulos.

### `contextManager.js`
Gestiona el historial de conversaci√≥n:
- `validateHistory()` - Valida el formato del historial
- `buildMessages()` - Construye el array de mensajes para Ollama

### `promptBuilder.js`
Construye los prompts con instrucciones del sistema:
- `getSystemInstructions()` - Retorna las instrucciones del sistema
- `buildPrompt()` - Crea prompts para modo sin contexto

### `responseFormatter.js`
Estandariza las respuestas de la API:
- `formatResponse()` - Formatea respuestas exitosas y de error

---

## üêõ Soluci√≥n de Problemas

### El contenedor de Ollama no inicia
```bash
docker logs ollama
```

### La API no responde
```bash
docker logs server
```

### Reiniciar todo
```bash
docker compose down
docker compose up -d --build
```

---

## üìÑ Licencia

Este proyecto es de c√≥digo abierto y est√° disponible bajo la licencia MIT.

---

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Haz fork del proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

---

## üë®‚Äçüíª Autores

**H√©ctor P. Osuna**  
GitHub: [@HectorPOsuna](https://github.com/HectorPOsuna)
